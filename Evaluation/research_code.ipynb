{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1>Generation Prompt from Process</h1>",
   "id": "9b357e9c"
  },
  {
   "cell_type": "code",
   "id": "549310ad",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ÌÖúÌîåÎ¶ø ÌååÏùº Î°úÎìú (utf-8 Í∏∞Ï§Ä)\n",
    "with open(\"FastMCP_Code_Generation_Prompt_EN.txt\", encoding=\"utf-8\") as f:\n",
    "    template = f.read()\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    \"\"\"ÌååÏùºÎ™ÖÏúºÎ°ú ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÏïàÏ†ÑÌïú Ïù¥Î¶ÑÏúºÎ°ú Î≥ÄÌôò\"\"\"\n",
    "    name = re.sub(r\"[^\\w\\s-]\", \"\", name)  # ÌïúÍ∏Ä Ìè¨Ìï® ÌóàÏö© Ïãú Ï†úÍ±∞\n",
    "    return name.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "\n",
    "def generate_prompt_from_template(template: str, process_group: str, idshorts: list[str]) -> str:\n",
    "    idshort_lines = \"\\n  - \" + \"\\n  - \".join(idshorts)\n",
    "    prompt = template.replace(\"- Process Group: milling_3_axis_v1\", f\"- Process Group: {process_group}\")\n",
    "    prompt = prompt.replace(\n",
    "        \"\"\"- spindleSpeedControl\n",
    "  - toolChangeRequest\n",
    "  - axisMovement\n",
    "  - coolantFlowToggle\n",
    "  - emergencyStopSignal\"\"\",\n",
    "        idshort_lines\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def generate_prompts(csv_path: str, output_dir: str = \"prompts_en\"):\n",
    "    df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        pg = row[\"Process Group\"]\n",
    "        ids = [s.strip() for s in str(row[\"Common idShorts\"]).split(\",\") if s.strip()]\n",
    "        content = generate_prompt_from_template(template, pg, ids)\n",
    "        filename = sanitize_filename(pg) + \".txt\"\n",
    "        prompt_path = Path(output_dir) / filename\n",
    "        prompt_path.write_text(content, encoding=\"utf-8\")\n",
    "        print(f\"‚úÖ Prompt generated: {prompt_path}\")\n",
    "\n",
    "# ÏÇ¨Ïö© ÏòàÏãú\n",
    "if __name__ == \"__main__\":\n",
    "    generate_prompts(\"valid_final.csv\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "98c2deeb",
   "metadata": {},
   "source": "<h1>Local Gemma3 Load Test</h1>"
  },
  {
   "cell_type": "code",
   "id": "4f3e2cfc",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def test_gemma_model_load(model_path: str = \"/home/h100-ku/gemma-3-27b-it\"):\n",
    "    try:\n",
    "        print(\"‚úÖ Load Tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        print(\"‚úÖ Load Model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"‚úÖ Success!\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Fail\", e)\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Ïã§Ìñâ\n",
    "test_gemma_model_load()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0fc5bb5",
   "metadata": {},
   "source": "<h1>Auto Code Generation Public AI</h1>"
  },
  {
   "cell_type": "code",
   "id": "814b4a5a",
   "metadata": {},
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# API ÌÇ§ Î∂àÎü¨Ïò§Í∏∞\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import openai\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ‚îÄ‚îÄ Ïú†Ìã∏ Ìï®Ïàò ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def clean_code_block(text: str) -> str:\n",
    "    code = re.sub(r\"^```(?:python)?\\n?\", \"\", text.strip())\n",
    "    return re.sub(r\"\\n?```$\", \"\", code)\n",
    "\n",
    "def save_code(code_text: str, model: str, process_group: str) -> str:\n",
    "    filename = f\"generated_{model}_{process_group}.py\"\n",
    "    Path(filename).write_text(clean_code_block(code_text), encoding=\"utf-8\")\n",
    "    return filename\n",
    "\n",
    "# ‚îÄ‚îÄ GPT (OpenAI) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def generate_from_gpt(prompt_file_path: str, process_group: str) -> str:\n",
    "    with open(prompt_file_path, encoding=\"utf-8\") as f:\n",
    "        prompt = f.read()\n",
    "\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"‚ùå OPENAI_API_KEY not set in .env file or environment variables.\")\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0.3,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        code = response.choices[0].message.content\n",
    "        return save_code(code, \"gpt4\", process_group)\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error GPT API:\", e)\n",
    "        raise\n",
    "\n",
    "# ‚îÄ‚îÄ Claude (Anthropic) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def generate_from_claude(prompt_file_path: str, process_group: str) -> str:\n",
    "    with open(prompt_file_path, encoding=\"utf-8\") as f:\n",
    "        prompt = f.read()\n",
    "\n",
    "    api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"‚ùå ANTHROPIC_API_KEY not set in .env file or environment variables.\")\n",
    "    client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-7-sonnet-20250219\",\n",
    "            max_tokens=4096,\n",
    "            temperature=0.3,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        code = response.content[0].text\n",
    "        return save_code(code, \"claude\", process_group)\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error Claude API:\", e)\n",
    "        raise\n",
    "\n",
    "# ‚îÄ‚îÄ Gemini (Google) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def generate_from_gemini(prompt_file_path: str, process_group: str) -> str:\n",
    "    with open(prompt_file_path, encoding=\"utf-8\") as f:\n",
    "        prompt = f.read()\n",
    "\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"‚ùå GOOGLE_API_KEY not set in .env file or environment variables.\")\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-2.5-pro-preview-05-06\")\n",
    "        response = model.generate_content(prompt)\n",
    "        code = response.text\n",
    "        return save_code(code, \"gemini\", process_group)\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error Gemini API:\", e)\n",
    "        raise\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1>Auto Code Generation Local AI</h1>",
   "id": "2696c9b7fd44e30c"
  },
  {
   "cell_type": "code",
   "id": "5995ae7a",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def clean_code_block(text: str) -> str:\n",
    "    code = re.sub(r\"^```(?:python)?\\n?\", \"\", text.strip())\n",
    "    return re.sub(r\"\\n?```$\", \"\", code)\n",
    "\n",
    "def save_code(code_text: str, model: str, process_group: str) -> str:\n",
    "    filename = f\"generated_{model}_{process_group}.py\"\n",
    "    Path(filename).write_text(clean_code_block(code_text), encoding=\"utf-8\")\n",
    "    return filename\n",
    "\n",
    "def load_gemma_model(model_path: str = \"/home/h100-ku/gemma-3-27b-it\"):\n",
    "    print(\"üîÅ Gemma Î™®Îç∏ Î°úÎî© Ï§ë...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"‚úÖ Gemma Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\")\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_with_loaded_gemma(prompt_file_path: str, process_group: str, tokenizer, model) -> str:\n",
    "    try:\n",
    "        with open(prompt_file_path, encoding=\"utf-8\") as f:\n",
    "            prompt = f.read()\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=8192,\n",
    "                temperature=0.3,\n",
    "                do_sample=True,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return save_code(code, \"gemma\", process_group)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Gemma ÏÉùÏÑ± Ïã§Ìå®:\", e)\n",
    "        raise\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f292c18",
   "metadata": {},
   "source": [
    "process_group = [\"PressPress_Servo_Type\",\"Heating_Heating_Quenching\",\"Rolling_Rolling_hot\",\"Heating_Heating__Aging\",\"AFPMMotorProductionType\"]\n",
    "prompt_path = [\"prompts_en/PressPress_Servo_Type.txt\",\"prompts_en/Heating_Heating_Quenching.txt\",\"prompts_en/Rolling_Rolling_hot.txt\",\"prompts_en/Heating_Heating_Aging.txt\",\"prompts_en/AFPMMotorProductionType.txt\"]\n",
    "\n",
    "for a in range(len(process_group)):\n",
    "    # Î™®Îç∏Î≥Ñ ÏΩîÎìú ÏÉùÏÑ± Î∞è Ï†ÄÏû•\n",
    "    print(\"request GPT\")\n",
    "    gpt_path = generate_from_gpt(prompt_path[a], process_group[a])\n",
    "    print(\"request GPT Complete\")\n",
    "    print(\"request Claude\")\n",
    "    claude_path = generate_from_claude(prompt_path[a], process_group[a])\n",
    "    print(\"request Claude Complete\")\n",
    "    print(\"request Gemini\")\n",
    "    gemini_path = generate_from_gemini(prompt_path[a], process_group[a])\n",
    "    print(\"request Gemini Complete\")\n",
    "\n",
    "print(\"‚úÖ GPT code saved:\", gpt_path)\n",
    "print(\"‚úÖ Claude code saved:\", claude_path)\n",
    "print(\"‚úÖ Gemini code saved:\", gemini_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0fc2e7c5",
   "metadata": {},
   "source": [
    "<h1>Evaluation Code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "id": "4011f923",
   "metadata": {},
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import anthropic\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ generate Evaluation prompt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def build_eval_prompt(code_text: str, process_group: str) -> str:\n",
    "    prompt_path = Path(f\"prompts_evaluate/{process_group}.txt\")\n",
    "    if not prompt_path.exists():\n",
    "        raise FileNotFoundError(f\"ÌîÑÎ°¨ÌîÑÌä∏ ÌååÏùº ÏóÜÏùå: {prompt_path}\")\n",
    "    with open(prompt_path, encoding=\"utf-8\") as f:\n",
    "        prompt_template = f.read()\n",
    "    return prompt_template.replace(\"{code}\", code_text.strip())\n",
    "\n",
    "\n",
    "#-----------Evaluate With Claude4 ---------------------------\n",
    "def evaluate_with_claude(code_path: str, process_group: str) -> str:\n",
    "    client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    code = Path(code_path).read_text(encoding=\"utf-8\")\n",
    "    prompt = build_eval_prompt(code, process_group)\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=8196,\n",
    "        temperature=0.2,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "#-----------Evaluate With O3 ---------------------------\n",
    "def evaluate_with_gpt_o3(code_path: str, process_group: str) -> str:\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    code = Path(code_path).read_text(encoding=\"utf-8\")\n",
    "    prompt = build_eval_prompt(code, process_group)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"o3-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Start Evaluate Function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def evaluate_all_for_process(process_group: str) -> dict:\n",
    "    code_models = [\"gpt4\", \"claude\", \"gemini\",\"gemma\"]  # ÏΩîÎìú ÏûëÏÑ± Î™®Îç∏\n",
    "    eval_models = [\"gpt4\", \"claude\"]            # ÌèâÍ∞Ä Î™®Îç∏ (o3-mini, claude)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for code_model in code_models:\n",
    "        filename = f\"generated_{code_model}_{process_group}.py\"\n",
    "        print(f\"\\n=== evaluation Code: {filename} ===\")\n",
    "\n",
    "        if not os.path.exists(filename):\n",
    "            print(\"‚ö†Ô∏è No Code File.\")\n",
    "            results[code_model] = \"No Code File\"\n",
    "            continue\n",
    "\n",
    "        evaluations = {}\n",
    "\n",
    "        for eval_model in eval_models:\n",
    "            try:\n",
    "                if eval_model == \"claude\":\n",
    "                    eval_result = evaluate_with_claude(filename, process_group)\n",
    "                elif eval_model == \"gpt4\":\n",
    "                    eval_result = evaluate_with_gpt_o3(filename, process_group)\n",
    "                else:\n",
    "                    eval_result = \"‚ùå Unsupported Model\"\n",
    "\n",
    "                evaluations[eval_model] = eval_result\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {eval_model.upper()} evaluate Exception:\", e)\n",
    "                evaluations[eval_model] = f\"evaluate Exception: {e}\"\n",
    "\n",
    "        # save evaluate Result\n",
    "        result_file = Path(f\"results/{process_group}_{code_model}_eval.txt\")\n",
    "        result_file.parent.mkdir(exist_ok=True)\n",
    "\n",
    "        # üìÑ add target Fila Name\n",
    "        header = f\"üìÑ Evaluate target: {filename}\\n\"\n",
    "        content = \"\\n\".join([\n",
    "            f\"\\n===== {eval.upper()} result =====\\n{res.strip()}\"\n",
    "            for eval, res in evaluations.items()\n",
    "        ])\n",
    "        result_file.write_text(header + content, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "        results[code_model] = evaluations\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_groups = [\n",
    "        \"PressPress_Servo_Type\",\n",
    "        \"Heating_Heating_Quenching\",\n",
    "        \"Rolling_Rolling_hot\",\n",
    "        \"Heating_Heating__Aging\",\n",
    "        \"AFPMMotorProductionType\"\n",
    "    ]\n",
    "    for pg in process_groups:\n",
    "        print(f\"\\nüìÇ Start Evaluate: {pg}\")\n",
    "        evaluate_all_for_process(pg)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "feddcf45",
   "metadata": {},
   "source": [
    "process_groups = [\"PressPress_Servo_Type\",\n",
    "        \"Heating_Heating_Quenching\",\n",
    "        \"Rolling_Rolling_hot\",\n",
    "        \"Heating_Heating__Aging\",\n",
    "        \"AFPMMotorProductionType\"]\n",
    "for pg in process_groups:\n",
    "    print(f\"\\nüìÇ Í≥µÏ†ï ÌèâÍ∞Ä ÏãúÏûë: {pg}\")\n",
    "    evaluate_all_for_process(pg)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fd1c3043",
   "metadata": {},
   "source": [
    "<h1>Heat Map</h1>"
   ]
  },
  {
   "cell_type": "code",
   "id": "15473fcd",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === 1. Input Result Data ===\n",
    "data = {\n",
    "    \"Task\": [],\n",
    "    \"Model\": [],\n",
    "    \"o3\": [],\n",
    "    \"Claude 4\": []\n",
    "}\n",
    "\n",
    "tasks = {\n",
    "    \"AFPMMotorProductionType\": [\n",
    "        (\"GPT 4o\", 54, 47),\n",
    "        (\"Claude 3.7 Sonnet\", 67, 45),\n",
    "        (\"Gemini 2.5 pro (preview)\", 86, 75),\n",
    "        (\"Gemma3 27B\", 92, 92)\n",
    "    ],\n",
    "    \"Heating_Aging\": [\n",
    "        (\"GPT 4o\", 57, 51),\n",
    "        (\"Claude 3.7 Sonnet\", 82, 75),\n",
    "        (\"Gemini 2.5 pro (preview)\", 72, 73),\n",
    "        (\"Gemma3 27B\", 95, 88)\n",
    "    ],\n",
    "    \"Heating_Quenching\": [\n",
    "        (\"GPT 4o\", 50, 47),\n",
    "        (\"Claude 3.7 Sonnet\", 60, 79),\n",
    "        (\"Gemini 2.5 pro (preview)\", 70, 73),\n",
    "        (\"Gemma3 27B\", 90, 94)\n",
    "    ],\n",
    "    \"PressPress_Servo_Type\": [\n",
    "        (\"GPT 4o\", 59, 51),\n",
    "        (\"Claude 3.7 Sonnet\", 66, 54),\n",
    "        (\"Gemini 2.5 pro (preview)\", 73, 73),\n",
    "        (\"Gemma3 27B\", 90, 85)\n",
    "    ],\n",
    "    \"Rolling_hot\": [\n",
    "        (\"GPT 4o\", 63, 53),\n",
    "        (\"Claude 3.7 Sonnet\", 76, 64),\n",
    "        (\"Gemini 2.5 pro (preview)\", 82, 73),\n",
    "        (\"Gemma3 27B\", 96, 80)\n",
    "    ]\n",
    "}\n",
    "\n",
    "for task, results in tasks.items():\n",
    "    for model, o3_score, claude4_score in results:\n",
    "        data[\"Task\"].append(task)\n",
    "        data[\"Model\"].append(model)\n",
    "        data[\"o3\"].append(o3_score)\n",
    "        data[\"Claude 4\"].append(claude4_score)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# === 2. HeatMap Function ===\n",
    "def plot_llm_evaluation_heatmaps(df: pd.DataFrame, save_path: str = None):\n",
    "    pivot_o3 = df.pivot(index=\"Model\", columns=\"Task\", values=\"o3\")\n",
    "    pivot_claude4 = df.pivot(index=\"Model\", columns=\"Task\", values=\"Claude 4\")\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(pivot_o3, annot=True, cmap=\"YlGnBu\", fmt=\"d\", cbar=True)\n",
    "    plt.title(\"Evaluation by Opean AI o3\")\n",
    "    plt.xlabel(\"Task\")\n",
    "    plt.ylabel(\"Model\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(pivot_claude4, annot=True, cmap=\"YlOrBr\", fmt=\"d\", cbar=True)\n",
    "    plt.title(\"Evaluation by Claude 4 Opus\")\n",
    "    plt.xlabel(\"Task\")\n",
    "    plt.ylabel(\"Model\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches=\"tight\")\n",
    "        print(f\"‚úÖ Success Save HeatMap Image: {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# === 3. Run ===\n",
    "plot_llm_evaluation_heatmaps(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b148b8e",
   "metadata": {},
   "source": "plot_llm_evaluation_heatmaps(df, save_path=\"llm_heatmap_highres.png\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manufactureData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
